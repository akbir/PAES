{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/am-khan/affect/blob/master/BOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "2LRnalZttT3k",
    "outputId": "0a7798c8-8443-4021-abd8-9abd3d3a2e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRG4dJgsQpgI"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b97c254f3e94814bcf79e714c5f4dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12819.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12818"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [] \n",
    "\n",
    "# FOR POCCA\n",
    "all_poems_file = '../PAES/poems/'\n",
    "for file in tqdm(os.listdir(all_poems_file)):\n",
    "    if file[-4:] == '.txt':\n",
    "        path = os.path.join(all_poems_file, file)\n",
    "        corpus.append(open(path).read())\n",
    "\n",
    "# # FOR FRIENDS\n",
    "# for file in ['data/Friends/train.csv', 'data/Friends/dev.csv']:\n",
    "#     with open(file) as f:\n",
    "#         df = pd.read_csv(f)\n",
    "#         corpus += list(df['content'])\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>interest</th>\n",
       "      <th>amusement</th>\n",
       "      <th>pride</th>\n",
       "      <th>joy</th>\n",
       "      <th>pleasure</th>\n",
       "      <th>contentment</th>\n",
       "      <th>love</th>\n",
       "      <th>admiration</th>\n",
       "      <th>relief</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>disgust</th>\n",
       "      <th>contempt</th>\n",
       "      <th>hate</th>\n",
       "      <th>anger</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>agreement</th>\n",
       "      <th>authors</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100 Bells</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>Tarfia Faizullah</td>\n",
       "      <td>My sister died. He raped me. They beat me. I f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Symmetry</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>Ari Banias</td>\n",
       "      <td>The magnolia before it blooms stands\\nbare as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After Noise</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>Sueyeun Juliette Lee</td>\n",
       "      <td>and who are you now       \\n       in this dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>Alicia Ostriker</td>\n",
       "      <td>The optimists among us\\ntaking heart because i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial Death</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>Roberto Harrison</td>\n",
       "      <td>back 90 suns to a run through the ferns\\nI rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>The House Gift</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.281128</td>\n",
       "      <td>Joanie Mackowski</td>\n",
       "      <td>Egg-white house, old\\nache in the rafters,\\nsm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>The Icehouse in Summer</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.465487</td>\n",
       "      <td>Howard Nemerov</td>\n",
       "      <td>A door sunk in a hillside, with a bolt\\nthick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>The Journey</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.052456</td>\n",
       "      <td>Yvor Winters</td>\n",
       "      <td>And then the dark fell and 'there has never'  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>The Lake</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.046172</td>\n",
       "      <td>Sophie Cabot Black</td>\n",
       "      <td>Day and night, the lake dreams of sky.\\nA priv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>The Lost Pilot</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.055752</td>\n",
       "      <td>James Tate</td>\n",
       "      <td>Your face did not rot\\nlike the others--the co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  interest  amusement  pride   joy  pleasure  \\\n",
       "0                 100 Bells      0.00       0.00    0.0  0.00      0.00   \n",
       "1                A Symmetry      0.23       0.27    0.0  0.03      0.07   \n",
       "2               After Noise      0.28       0.02    0.0  0.00      0.00   \n",
       "3                     April      0.15       0.20    0.0  0.35      0.15   \n",
       "4          Artificial Death      0.00       0.00    0.0  0.00      0.03   \n",
       "..                      ...       ...        ...    ...   ...       ...   \n",
       "266          The House Gift      0.50       0.30    0.1  0.20      0.10   \n",
       "267  The Icehouse in Summer      0.60       0.30    0.2  0.40      0.50   \n",
       "268             The Journey      0.50       0.40    0.3  0.40      0.30   \n",
       "269                The Lake      0.50       0.60    0.4  0.30      0.30   \n",
       "270          The Lost Pilot      0.40       0.30    0.2  0.20      0.10   \n",
       "\n",
       "     contentment  love  admiration  relief  ...  fear  disgust  contempt  \\\n",
       "0           0.00  0.00        0.00    0.00  ...  0.13     0.43      0.17   \n",
       "1           0.10  0.07        0.00    0.00  ...  0.10     0.10      0.00   \n",
       "2           0.10  0.00        0.10    0.00  ...  0.00     0.00      0.00   \n",
       "3           0.15  0.00        0.00    0.10  ...  0.00     0.20      0.30   \n",
       "4           0.10  0.00        0.23    0.13  ...  0.10     0.00      0.00   \n",
       "..           ...   ...         ...     ...  ...   ...      ...       ...   \n",
       "266         0.10  0.20        0.20    0.20  ...  0.30     0.20      0.10   \n",
       "267         0.40  0.40        0.40    0.50  ...  0.30     0.30      0.30   \n",
       "268         0.30  0.10        0.20    0.20  ...  0.70     0.50      0.50   \n",
       "269         0.30  0.20        0.40    0.40  ...  0.20     0.20      0.40   \n",
       "270         0.10  0.30        0.30    0.30  ...  0.10     0.10      0.10   \n",
       "\n",
       "     hate  anger  valence  arousal  agreement               authors  \\\n",
       "0    0.13   0.43    -0.10     0.06   0.480000      Tarfia Faizullah   \n",
       "1    0.00   0.00     0.01     0.03   0.130000            Ari Banias   \n",
       "2    0.00   0.00    -0.01    -0.04   0.240000  Sueyeun Juliette Lee   \n",
       "3    0.00   0.00     0.03     0.06  -0.130000       Alicia Ostriker   \n",
       "4    0.00   0.00     0.00    -0.06   0.380000      Roberto Harrison   \n",
       "..    ...    ...      ...      ...        ...                   ...   \n",
       "266  0.10   0.20     0.00     0.00   0.281128      Joanie Mackowski   \n",
       "267  0.20   0.40     0.10    -0.00   0.465487        Howard Nemerov   \n",
       "268  0.40   0.30    -0.20    -0.00  -0.052456          Yvor Winters   \n",
       "269  0.30   0.30     0.00     0.00   0.046172    Sophie Cabot Black   \n",
       "270  0.00   0.00     0.00    -0.10  -0.055752            James Tate   \n",
       "\n",
       "                                               content  \n",
       "0    My sister died. He raped me. They beat me. I f...  \n",
       "1    The magnolia before it blooms stands\\nbare as ...  \n",
       "2    and who are you now       \\n       in this dif...  \n",
       "3    The optimists among us\\ntaking heart because i...  \n",
       "4    back 90 suns to a run through the ferns\\nI rem...  \n",
       "..                                                 ...  \n",
       "266  Egg-white house, old\\nache in the rafters,\\nsm...  \n",
       "267  A door sunk in a hillside, with a bolt\\nthick ...  \n",
       "268  And then the dark fell and 'there has never'  ...  \n",
       "269  Day and night, the lake dreams of sky.\\nA priv...  \n",
       "270  Your face did not rot\\nlike the others--the co...  \n",
       "\n",
       "[271 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FOR POCCA\n",
    "DATA_DIR = os.path.abspath('POCCA/')\n",
    "\n",
    "# FOR FRIENDS\n",
    "# DATA_DIR = os.path.abspath('data/Friends')\n",
    "def get_datasets(data_dir: str):\n",
    "    datasets = {}\n",
    "    for partition in [\"train\",\"test\"]:\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, f'{partition}.csv'))\n",
    "        datasets[partition] = df\n",
    "    return datasets   \n",
    "\n",
    "datasets = get_datasets(DATA_DIR)\n",
    "datasets['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 72123\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(f'vocab size: {X.toarray().shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 72123)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(['hello this is a document', 'this is another']).toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WwbG_cqfOPIf",
    "outputId": "431d3a5c-8acd-409f-b3d1-09b7b3d4154d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "poems = datasets['train']['content']\n",
    "\n",
    "# FOR POCCA\n",
    "def get_sadness_values(df):\n",
    "    array = df['sadness'].to_numpy()\n",
    "    return np.where(array>0.3, 1, 0)\n",
    "\n",
    "def get_valence_values(df):\n",
    "    array = df['arousal'].to_numpy()\n",
    "    return np.where(array>0, 1, 0)\n",
    "\n",
    "def get_affect_regression(df):\n",
    "    array = np.asarray([df['arousal'],df['valence']])\n",
    "    return array.T\n",
    "\n",
    "# FOR FRIENDS\n",
    "def get_label(df):\n",
    "    return np.asarray(df['label'])\n",
    "\n",
    "\n",
    "def get_agreement(df):\n",
    "    return np.asarray(df['agreement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5e2GZTO02FY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271, 72123) (271,)\n",
      "(68, 72123)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.7058823529411765)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "labels = get_valence_values\n",
    "\n",
    "X_train = vectorizer.transform(datasets['train']['content'])\n",
    "Y_train = labels(datasets['train'])\n",
    "\n",
    "X_test = vectorizer.transform(datasets['test']['content'])\n",
    "Y_test = labels(datasets['test'])\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_predictions = clf.predict(X_test)\n",
    "f1_score(Y_test, Y_predictions), accuracy_score(Y_test, Y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5e2GZTO02FY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271, 72123) (271, 2)\n",
      "(68, 72123) (68, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1285076037691133"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "labels = get_affect_regression\n",
    "\n",
    "X_train = vectorizer.transform(datasets['train']['content'])\n",
    "Y_train = labels(datasets['train'])*100\n",
    "X_test = vectorizer.transform(datasets['test']['content'])\n",
    "Y_test = labels(datasets['test'])*100\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "clf = GaussianNB()\n",
    "Y_1 =  (Y_train[:,0])\n",
    "clf.fit(X_train.toarray(), Y_1.astype(int))\n",
    "Y_pred = clf.predict(X_test.toarray())\n",
    "\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(X_train.toarray(), Y_train[:,1].astype(int))\n",
    "Y_pred1 = clf1.predict(X_test.toarray())\n",
    "\n",
    "Y_pred = np.stack([Y_pred, Y_pred1], axis=-1)\n",
    "\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.937266468185301"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "labels = get_agreement\n",
    "X_train = vectorizer.transform(datasets['train']['content'])\n",
    "Y_train = labels(datasets['train'])*100\n",
    "X_test = vectorizer.transform(datasets['test']['content'])\n",
    "Y_test = labels(datasets['test'])*100\n",
    "\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train.toarray(), Y_train.astype(int))\n",
    "Y_pred = clf.predict(X_test.toarray())\n",
    "\n",
    "\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17360143 0.1363708  0.13508032 0.13267839 0.13584559 0.12462874\n",
      " 0.118868   0.12263689 0.12833155 0.12323239 0.11928421 0.12911806\n",
      " 0.12246143 0.14045255 0.12066204 0.15137224 0.15664313 0.12131465\n",
      " 0.13111263 0.14161968 0.13032109 0.11825588 0.1502283  0.12545985\n",
      " 0.18479262 0.13273704 0.12721439 0.10367145 0.14953232 0.1153099\n",
      " 0.16138302 0.14329987 0.11965157 0.16355637 0.12675128 0.14537805\n",
      " 0.15595305 0.13546984 0.16193436 0.0943947  0.14065102 0.12202631\n",
      " 0.1401813  0.1404608  0.15070951 0.13554872 0.1027411  0.13820544\n",
      " 0.12917506 0.12272706 0.13756466 0.13305628 0.14185996 0.1598903\n",
      " 0.11505355 0.14452019 0.11255087 0.14788757 0.11744001 0.13168944\n",
      " 0.13862449 0.1329071  0.09533359 0.12495422 0.10957039 0.18235641\n",
      " 0.14846088 0.1456744 ]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeH2YNjUBCdTKKi0rwMgyX",
   "include_colab_link": true,
   "name": "BOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
